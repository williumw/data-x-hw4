{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import log"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. So my method behind this is using the following: \n",
    "\n",
    "    1. Compute entropy and info gain for each attribute.\n",
    "    2. Sort the attributes in ascending order w.r.t. entropy (descending order for info gain)\n",
    "    3. While attribute list is not empty or length(attribute list)>threshold, select the first attribute to be the root of the tree/subtree. \n",
    "    4. Remove the attribute from the sorted list.\n",
    "    5. Goto step 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Defaulter  HasFamily  HasJob  IsAbove30years\n",
      "0          0          1       1               1\n",
      "1          0          1       1               1\n",
      "2          0          0       1               1\n",
      "3          0          1       0               0\n",
      "4          1          0       0               1\n",
      "5          1          1       0               0\n",
      "6          1          0       1               1\n",
      "7          1          0       1               1\n"
     ]
    }
   ],
   "source": [
    "d = {'HasJob': [1,1,1,0,0,0,1,1], 'HasFamily': [1,1,0,1,0,1,0,0], \n",
    "     'IsAbove30years' : [1,1,1,0,1,0,1,1], 'Defaulter' : [0,0,0,0,1,1,1,1]}\n",
    "\n",
    "df = pd.DataFrame(data=d)\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root Error: 0.5\n",
      "HasJob Split: 0.375\n",
      "HasFamily Split: 0.25\n",
      "IsAbove30years Split: 0.5\n"
     ]
    }
   ],
   "source": [
    "#define feature selection\n",
    "def root_error(label):\n",
    "    error = 0\n",
    "    for i in range(len(df)):\n",
    "        if df.get(label)[i] == 1:\n",
    "            error += 1\n",
    "    return error / len(df)\n",
    "\n",
    "def classification_error(feature, label):\n",
    "    errors = 0\n",
    "    for i in range(len(df)):\n",
    "        if df.get(feature)[i] == df.get(label)[i]:\n",
    "            errors += 1\n",
    "    return errors / len(df)\n",
    "\n",
    "print(\"Root Error:\", root_error('Defaulter'))\n",
    "print(\"HasJob Split:\", classification_error('HasJob', 'Defaulter'))\n",
    "print(\"HasFamily Split:\", classification_error('HasFamily', 'Defaulter'))\n",
    "print(\"IsAbove30years Split:\", classification_error('IsAbove30years', 'Defaulter'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From this information, HasFamily is the best feature as our first split because it provides the lowest error rating."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8018185525433372\n"
     ]
    }
   ],
   "source": [
    "#Entropy and Source Coding Theorem\n",
    "# probs = [0.7, 0.2, 0.1]\n",
    "\n",
    "a = 0.7\n",
    "b = 0.2\n",
    "c = 0.1\n",
    "\n",
    "entropy = (-a) * np.log(a) + (-b) * np.log(b) + (-c) * np.log(c)\n",
    "\n",
    "\n",
    "print(entropy)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "According to Shannon's Source Coding Theorem, the entropy of 0.8 means that if we were to compress information given about these probabilities, we would need 0.8 bits to do so. If we were to use any less bits, we will lose information, but if we use more bits, we will not lose any information."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Natural Language Processing:"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.\n",
    "The bag-of-words model is a model of representing textual data with machine learning algorithms. Simply put, it is a way of extracting features from text for use in modeling. The approach starts with a vocabulary of known words and a measure of the presence of known words. That is where the “bag” part of the name comes from. Any other information about the order or structure of words in the document is discarded when doing analysis. The limitations of bag-of-words comes with the vocabulary. The vocabulary requires careful design in order to measure the size (which can affect the document’s representation). Sparse representations are harder to model. When words are discarded, it can change the context of the original document.\n",
    "\n",
    "Word2vec uses two-layer neural networks that are trained to reconstruct linguistic contexts of words. It uses a large corpus of text as its input and produces a vector space with hundreds of dimensions having assigned each unique word to a space within the dimensions. Word2vec utilizes two major models architectures to produce a distributed representation of words: continuous bag-of-words, or continuous skip-gram. Continuous bag-of-words has the model predict the current word from a window of surrounding context words. Continuous skip-gram uses the current word to predict the surrounding window of context words.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.\n",
    "\n",
    "Word vector: the encoding of a given word is set to 1 where all other elements are 0.\n",
    "Word embedding: word embeddings are the texts converted into numbers.\n",
    "\n",
    "Word embedding of a word depends on the way the dictionary is prepared because in real world applications, we might have a corpus which contains millions of documents with millions of unique words. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3 . \n",
    "\n",
    "Corpus in NLP: a corpus is a large collection of texts. It is a body of written or spoken material upon which a linguistic analysis is based."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/William/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /Users/William/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/wordnet.zip.\n"
     ]
    }
   ],
   "source": [
    "import bs4 as bs\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize # tokenizes sentences\n",
    "#  >>> import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "import re\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tag import pos_tag\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.util import ngrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def review_cleaner(review,lemmatize=True,stem=False):\n",
    "    '''\n",
    "    Clean and preprocess a review.\n",
    "\n",
    "    1. Remove HTML tags\n",
    "    2. Use regex to remove all special characters (only keep letters)\n",
    "    3. Make strings to lower case and tokenize / word split reviews\n",
    "    4. Remove English stopwords\n",
    "    5. Rejoin to one string\n",
    "    '''\n",
    "    ps = PorterStemmer()\n",
    "    wnl = WordNetLemmatizer()\n",
    "\n",
    "    #2. Remove punctuation\n",
    "    review = re.sub(\"[^a-zA-Z]\", \" \",review)\n",
    "    \n",
    "    #3. Tokenize into words (all lower case)\n",
    "    review = review.lower().split()\n",
    "    \n",
    "    #4.Set stopwords\n",
    "    eng_stopwords = set(stopwords.words(\"english\"))\n",
    "\n",
    "    clean_review=[]\n",
    "    for word in review:\n",
    "        if word not in eng_stopwords:\n",
    "            if lemmatize is True:\n",
    "                word=wnl.lemmatize(word)\n",
    "            elif stem is True:\n",
    "                if word == 'oed':\n",
    "                    continue\n",
    "                word=ps.stem(word)\n",
    "            clean_review.append(word)\n",
    "    return(clean_review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentences</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>It is a truth universally acknowledged, that a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>\"My dear Mr. Bennet,\" said his lady to him one...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Bennet replied that he had not.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>\"But it is,\" returned she; \"for Mrs. Long has ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Bennet made no answer.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           sentences\n",
       "0  It is a truth universally acknowledged, that a...\n",
       "1  \"My dear Mr. Bennet,\" said his lady to him one...\n",
       "2                    Bennet replied that he had not.\n",
       "3  \"But it is,\" returned she; \"for Mrs. Long has ...\n",
       "4                             Bennet made no answer."
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = pd.read_csv('prideNprejudice.csv')\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = len(train['sentences'])\n",
    "\n",
    "sentences = []\n",
    "\n",
    "for i in range(n):\n",
    "    sentences.append(review_cleaner(train['sentences'][i]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training word2vec model... \n"
     ]
    }
   ],
   "source": [
    "# Set values for various parameters\n",
    "num_features = 300    # Word vector dimensionality                      \n",
    "min_word_count = 40   # ignore all words with total frequency lower than this                       \n",
    "num_workers = 4       # Number of threads to run in parallel\n",
    "context = 10         # Context window size                                                                                    \n",
    "\n",
    "\n",
    "# Initialize and train the model (this will take some time)\n",
    "from gensim.models import word2vec\n",
    "\n",
    "print(\"Training word2vec model... \")\n",
    "model = word2vec.Word2Vec(sentences, workers=num_workers, \\\n",
    "           size=num_features, min_count = min_word_count, \\\n",
    "            window = context, iter=28)\n",
    "\n",
    "\n",
    "# save the model for later use. You can load it later using Word2Vec.load()\n",
    "model_name = \"300features_40minwords_10context\"\n",
    "model.save(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/William/miniconda3/lib/python3.6/site-packages/ipykernel_launcher.py:5: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  \"\"\"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.figure.Figure at 0x116d89160>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn import decomposition\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "vocab_tmp = list(model.wv.vocab)\n",
    "X = model[vocab_tmp]\n",
    "# get two principle components of the feature space\n",
    "pca= decomposition.PCA(n_components=2).fit_transform(X)\n",
    "\n",
    "# set figure settings\n",
    "plt.figure(figsize=(10,10),dpi=100)\n",
    "\n",
    "# save pca values and vocab in dataframe df\n",
    "df = pd.concat([pd.DataFrame(pca),pd.Series(vocab_tmp)],axis=1)\n",
    "df.columns = ['x', 'y', 'word']\n",
    "\n",
    "\n",
    "\n",
    "plt.xlabel(\"Ist principal component\")\n",
    "plt.ylabel('2nd principal component')\n",
    "\n",
    "\n",
    "plt.scatter(x=pca[:, 0], y=pca[:, 1],s=3)\n",
    "for i, word in enumerate(df['word'][0:100]):\n",
    "    plt.annotate(word, (df['x'].iloc[i], df['y'].iloc[i]))\n",
    "plt.title(\"PCA Embedding\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity (elizabeth, girl):  0.6491067196921319\n",
      "Similarity (family, lydia):  0.849229730431287\n",
      "Similarity (sir, lady):  0.7474620225046102\n",
      "Similarity (william, enough):  0.1977049622767834\n",
      "Similarity (may, sister):  0.5857893911449343\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/William/miniconda3/lib/python3.6/site-packages/ipykernel_launcher.py:1: DeprecationWarning: Call to deprecated `similarity` (Method will be removed in 4.0.0, use self.wv.similarity() instead).\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n",
      "/Users/William/miniconda3/lib/python3.6/site-packages/ipykernel_launcher.py:2: DeprecationWarning: Call to deprecated `similarity` (Method will be removed in 4.0.0, use self.wv.similarity() instead).\n",
      "  \n",
      "/Users/William/miniconda3/lib/python3.6/site-packages/ipykernel_launcher.py:3: DeprecationWarning: Call to deprecated `similarity` (Method will be removed in 4.0.0, use self.wv.similarity() instead).\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n",
      "/Users/William/miniconda3/lib/python3.6/site-packages/ipykernel_launcher.py:4: DeprecationWarning: Call to deprecated `similarity` (Method will be removed in 4.0.0, use self.wv.similarity() instead).\n",
      "  after removing the cwd from sys.path.\n",
      "/Users/William/miniconda3/lib/python3.6/site-packages/ipykernel_launcher.py:5: DeprecationWarning: Call to deprecated `similarity` (Method will be removed in 4.0.0, use self.wv.similarity() instead).\n",
      "  \"\"\"\n"
     ]
    }
   ],
   "source": [
    "print(\"Similarity (elizabeth, girl): \", model.similarity('elizabeth','girl'))\n",
    "print(\"Similarity (family, lydia): \",model.similarity('family','lydia'))\n",
    "print(\"Similarity (sir, lady): \",model.similarity('sir','lady'))\n",
    "print(\"Similarity (william, enough): \",model.similarity('william','enough'))\n",
    "print(\"Similarity (may, sister): \",model.similarity('may','sister'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab length: 234\n"
     ]
    }
   ],
   "source": [
    "vocab_tmp = list(model.wv.vocab)\n",
    "print('Vocab length:',len(vocab_tmp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/William/miniconda3/lib/python3.6/site-packages/ipykernel_launcher.py:1: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "300"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(model['girl'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. word2vec model for Pride and Prejudice"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: SQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "connection = sqlite3.connect('dogs.db')\n",
    "cursor = connection.cursor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<sqlite3.Cursor at 0x10cb209d0>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cursor.execute('DROP TABLE IF EXISTS parents')\n",
    "sql_command = \"\"\"CREATE TABLE parents (\n",
    "    parent VARCHAR(20),\n",
    "    child VARCHAR(20));\"\"\"\n",
    "cursor.execute(sql_command)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<sqlite3.Cursor at 0x10cb209d0>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sql_command = \"\"\"INSERT INTO parents (parent, child)\n",
    "    VALUES (\"abraham\", \"barack\") UNION\n",
    "    VALUES (\"abraham\", \"clinton\") UNION\n",
    "    VALUES (\"delano\", \"herbert\") UNION\n",
    "    VALUES (\"fillmore\", \"abraham\") UNION\n",
    "    VALUES (\"fillmore\", \"delano\") UNION\n",
    "    VALUES (\"fillmore\", \"grover\") UNION\n",
    "    VALUES (\"eisenhower\", \"fillmore\");\"\"\"\n",
    "cursor.execute(sql_command)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "connection.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "connection.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Simple Selects on Parent Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "connection = sqlite3.connect('dogs.db')\n",
    "cursor = connection.cursor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('abraham', 'barack')\n",
      "('abraham', 'clinton')\n",
      "('delano', 'herbert')\n",
      "('eisenhower', 'fillmore')\n",
      "('fillmore', 'abraham')\n",
      "('fillmore', 'delano')\n",
      "('fillmore', 'grover')\n"
     ]
    }
   ],
   "source": [
    "parents = cursor.execute('SELECT * from parents')\n",
    "for parent in parents.fetchall(): \n",
    "    print(parent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Select child and parent where abraham is the parent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('barack', 'abraham')\n",
      "('clinton', 'abraham')\n"
     ]
    }
   ],
   "source": [
    "abe = cursor.execute('SELECT child, parent '\n",
    "                   'FROM parents '\n",
    "                   'WHERE parent = \"abraham\"')\n",
    "for row in abe.fetchall(): \n",
    "    print(row)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Joins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#setup\n",
    "cursor.execute('DROP TABLE IF EXISTS dogs')\n",
    "sql_command = \"\"\"CREATE TABLE dogs AS\n",
    " SELECT \"abraham\" AS name, \"long\" AS fur UNION\n",
    " SELECT \"barack\", \"short\" UNION\n",
    " SELECT \"clinton\", \"long\" UNION\n",
    " SELECT \"delano\", \"long\" UNION\n",
    " SELECT \"eisenhower\", \"short\" UNION\n",
    " SELECT \"fillmore\", \"curly\" UNION\n",
    " SELECT \"grover\", \"short\" UNION\n",
    " SELECT \"herbert\", \"curly\";\"\"\"\n",
    "cursor.execute(sql_command)\n",
    "connection.commit()\n",
    "connection.close()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "1. Count the number of short haired dogs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3,)\n"
     ]
    }
   ],
   "source": [
    "connection = sqlite3.connect('dogs.db')\n",
    "cursor = connection.cursor()\n",
    "fur = cursor.execute('SELECT COUNT (*) '\n",
    "                   'FROM dogs '\n",
    "                   'WHERE fur = \"short\"')\n",
    "for row in fur.fetchall(): \n",
    "    print(row)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "2. Join tables parents and dogs and select the parents of curly dogs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('eisenhower',)\n",
      "('delano',)\n"
     ]
    }
   ],
   "source": [
    "curly = cursor.execute('SELECT parent '\n",
    "                   'FROM parents '\n",
    "                   'JOIN dogs ON parents.child = dogs.name '\n",
    "                   'WHERE dogs.fur = \"curly\"')\n",
    "for row in curly.fetchall(): \n",
    "    print(row)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "3. Join tables parents and dogs, and select the parents and children that have the same fur type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('abraham', 'clinton')\n"
     ]
    }
   ],
   "source": [
    "furtype = cursor.execute('SELECT parent, child '\n",
    "                   'FROM parents '\n",
    "                   'JOIN dogs a ON a.name = parents.child '\n",
    "                   'JOIN dogs b ON b.name = parents.parent '\n",
    "                   'WHERE a.fur = b.fur'\n",
    "                   )\n",
    "for row in furtype.fetchall(): \n",
    "    print(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "connection.commit()\n",
    "connection.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Aggregate functions, numerical logic and grouping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "connection = sqlite3.connect('dogs.db')\n",
    "cursor = connection.cursor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<sqlite3.Cursor at 0x10cc33c00>"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#setup\n",
    "cursor.execute('DROP TABLE IF EXISTS animals')\n",
    "sql_command = \"\"\"create table animals as\n",
    " select \"dog\" as kind, 4 as legs, 20 as weight union\n",
    " select \"cat\" , 4 , 10 union\n",
    " select \"ferret\" , 4 , 10 union\n",
    " select \"parrot\" , 2 , 6 union\n",
    " select \"penguin\" , 2 , 10 union\n",
    "select \"t-rex\" , 2 , 12000;\"\"\"\n",
    "cursor.execute(sql_command)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "connection.commit()\n",
    "connection.close()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "1. Select the animal with the minimum weight. Display kind and min weight."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "connection = sqlite3.connect('dogs.db')\n",
    "cursor = connection.cursor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('parrot', 6)\n"
     ]
    }
   ],
   "source": [
    "weight = cursor.execute('SELECT kind, MIN(weight) '\n",
    "                   'FROM animals ')\n",
    "for row in weight.fetchall(): \n",
    "    print(row)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "2. Use the aggregate function AVG to display a table with the average number of legs and average weight."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3.0, 2009.3333333333333)\n"
     ]
    }
   ],
   "source": [
    "avgweight = cursor.execute('SELECT AVG(legs), AVG(weight) '\n",
    "                   'FROM animals ')\n",
    "for row in avgweight.fetchall(): \n",
    "    print(row)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "3. SELECT the animal kind(s) that have more than two legs, but weighs less than 20. Display kind, weight, legs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('cat', 10, 4)\n",
      "('ferret', 10, 4)\n"
     ]
    }
   ],
   "source": [
    "legs = cursor.execute('SELECT kind, weight, legs '\n",
    "                   'FROM animals '\n",
    "                   'WHERE legs > 2 '\n",
    "                   'AND weight < 20')\n",
    "for row in legs.fetchall(): \n",
    "    print(row)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "4. SELECT the average weight for all the animals with 2 legs and the animals with 4 legs(by using GROUP BY)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4005.3333333333335,)\n",
      "(13.333333333333334,)\n"
     ]
    }
   ],
   "source": [
    "weight = cursor.execute('SELECT AVG(weight) '\n",
    "                   'FROM animals '\n",
    "                   'GROUP BY legs')\n",
    "for row in weight.fetchall(): \n",
    "    print(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
